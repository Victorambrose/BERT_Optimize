{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook deals with SORTING | workflow (1) : Deduplicate --> Sort | workflow (1) : Normalize --> Deduplicate --> Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:49:38.205741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744197578.227212  822642 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744197578.233931  822642 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744197578.249081  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744197578.249102  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744197578.249105  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744197578.249106  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 16:49:38.254562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5,6'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose three models , legal_BERT , legal_longformer , legal_Roberta\n",
    "## datasets :\n",
    "\n",
    "1) Original SCOTUS \n",
    "2) Dedup and sort \n",
    "3) Normalize Deduplicate and Sort the words bassed on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800d86e5b6cd44c1bd4dbab4ad3216f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d354ffb0d1f04c29b526f120d1d57508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/41.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b4b1c2c836401c80156cbf708c0d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8446535d604a82970951ba1d978ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367867ecc0f0445a9f6037f2b80e1acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25f3382c60340dc8e69d41363efb709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842b07954fb54b3c84ccd7644f485693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training Legal BERT original dataset\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_822642/2127134393.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 12:06, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.279800</td>\n",
       "      <td>2.004190</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.111831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.290700</td>\n",
       "      <td>1.266581</td>\n",
       "      <td>0.647857</td>\n",
       "      <td>0.378878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.984531</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.452076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>0.738571</td>\n",
       "      <td>0.604348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.843505</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.655031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.834876</td>\n",
       "      <td>0.766429</td>\n",
       "      <td>0.686413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>0.872613</td>\n",
       "      <td>0.776429</td>\n",
       "      <td>0.696071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.979551</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.698415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>1.033457</td>\n",
       "      <td>0.766429</td>\n",
       "      <td>0.689449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>1.100302</td>\n",
       "      <td>0.776429</td>\n",
       "      <td>0.708050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>1.124450</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.709177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>1.200345</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.713548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.205702</td>\n",
       "      <td>0.787143</td>\n",
       "      <td>0.718726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>1.293257</td>\n",
       "      <td>0.782143</td>\n",
       "      <td>0.711777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.300109</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.717673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.325153</td>\n",
       "      <td>0.783571</td>\n",
       "      <td>0.713024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.343326</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.714344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.371480</td>\n",
       "      <td>0.783571</td>\n",
       "      <td>0.711959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.368167</td>\n",
       "      <td>0.782857</td>\n",
       "      <td>0.712060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.369270</td>\n",
       "      <td>0.784286</td>\n",
       "      <td>0.714250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Evaluation Metrics: {'eval_loss': 1.2057017087936401, 'eval_micro_f1': 0.7871428571428571, 'eval_macro_f1': 0.7187258867162242, 'eval_runtime': 4.1509, 'eval_samples_per_second': 337.277, 'eval_steps_per_second': 8.432, 'epoch': 20.0}\n",
      "--------------------------------------------------\n",
      "Training Legal BERT dedup_and_sort\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4338f6464464219a300f5a51c8fc03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48d33e1f464b3b88ae86d3a0005962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377cdfd3664d4bbb826e1bedfc36e0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_822642/2127134393.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 11:53, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.159900</td>\n",
       "      <td>1.902084</td>\n",
       "      <td>0.440714</td>\n",
       "      <td>0.134928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.390161</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.355136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.071700</td>\n",
       "      <td>1.237594</td>\n",
       "      <td>0.629286</td>\n",
       "      <td>0.362497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>1.091375</td>\n",
       "      <td>0.675714</td>\n",
       "      <td>0.465638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.678900</td>\n",
       "      <td>1.032162</td>\n",
       "      <td>0.695714</td>\n",
       "      <td>0.548376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>1.056544</td>\n",
       "      <td>0.712143</td>\n",
       "      <td>0.546608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>1.058330</td>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.569711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>1.150014</td>\n",
       "      <td>0.707857</td>\n",
       "      <td>0.598516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>1.177957</td>\n",
       "      <td>0.719286</td>\n",
       "      <td>0.613777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>1.265226</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.621169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.100700</td>\n",
       "      <td>1.271758</td>\n",
       "      <td>0.727857</td>\n",
       "      <td>0.626356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>1.417074</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.643126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>1.461334</td>\n",
       "      <td>0.722857</td>\n",
       "      <td>0.636724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>1.586111</td>\n",
       "      <td>0.721429</td>\n",
       "      <td>0.634045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>1.636740</td>\n",
       "      <td>0.712857</td>\n",
       "      <td>0.617034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>1.641945</td>\n",
       "      <td>0.720714</td>\n",
       "      <td>0.626094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.702760</td>\n",
       "      <td>0.723571</td>\n",
       "      <td>0.615096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.647359</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.638168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.674440</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.633104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.684557</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.634880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Evaluation Metrics: {'eval_loss': 1.4170739650726318, 'eval_micro_f1': 0.7321428571428571, 'eval_macro_f1': 0.643125976839279, 'eval_runtime': 4.3267, 'eval_samples_per_second': 323.569, 'eval_steps_per_second': 8.089, 'epoch': 20.0}\n",
      "--------------------------------------------------\n",
      "Training Legal BERT norm_dedup_sort\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c0653c31b3450ea226244832007a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f00ff77e2f34de8b56fd62c967b0898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d02fb31b64cf7b52f9e7974961c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_822642/2127134393.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='876' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 876/2500 04:07 < 07:39, 3.53 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.159300</td>\n",
       "      <td>1.895596</td>\n",
       "      <td>0.442143</td>\n",
       "      <td>0.132168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>1.384931</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.353831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.069800</td>\n",
       "      <td>1.226600</td>\n",
       "      <td>0.636429</td>\n",
       "      <td>0.365991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>1.101609</td>\n",
       "      <td>0.682857</td>\n",
       "      <td>0.484806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>1.028835</td>\n",
       "      <td>0.703571</td>\n",
       "      <td>0.566915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>1.084986</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.542870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.399600</td>\n",
       "      <td>1.065645</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.569042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_names = {\n",
    "    \"legal_BERT\": \"nlpaueb/legal-bert-base-uncased\",\n",
    "    \"legal_longformer\": \"lexlms/legal-longformer-base\",\n",
    "    \"legal_Roberta\":\"lexlms/legal-roberta-base\"\n",
    "}\n",
    "\n",
    "learning_rate = 3e-5\n",
    "epochs = 20\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "# ----- Load Dataset -----\n",
    "original_dataset=load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n",
    "dedup_and_sort = load_dataset(\"victorambrose11/scotus_deduplicate_sort\")\n",
    "norm_dedup_sort = load_dataset(\"victorambrose11/scotus_normalize_deduplicate_sort\")\n",
    "\n",
    "label_list = original_dataset[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# ----- Token Classification Metric -----\n",
    "def compute_f1(pred):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ----- Training Function -----\n",
    "def train_transformer_model(model_key, dataset, dataset_label):\n",
    "    model_checkpoint = model_names[model_key]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "    # Preprocessing\n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    encoded_dataset = dataset.map(preprocess, batched=True)\n",
    "    encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "    encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_key}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        seed=seed,\n",
    "        logging_dir=f\"./logs_{model_key}\",\n",
    "        logging_steps=50,\n",
    "        warmup_steps=500,\n",
    "        lr_scheduler_type=\"linear\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        compute_metrics=compute_f1,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"DEBUG: Evaluation Metrics: {metrics}\")\n",
    "    results[f\"{model_key} ({dataset_label})\"] = {\n",
    "        \"Micro F1\": round(metrics.get(\"eval_micro_f1\", 0.0) * 100, 2),\n",
    "        \"Macro F1\": round(metrics.get(\"eval_macro_f1\", 0.0) * 100, 2),\n",
    "        \"Dataset\": dataset_label\n",
    "    }\n",
    "# ----- Train LegalBERT and LegalLongformer -----\n",
    "# original_dataset\n",
    "# tfidf_srt_original_dataset \n",
    "# tfidf_srt_normalized_dataset\n",
    "\n",
    "## legal bert on all three datasets \n",
    "print('-'*50)\n",
    "print('Training Legal BERT original dataset')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_BERT\", original_dataset, \"Original\")\n",
    "print('-'*50)\n",
    "print('Training Legal BERT dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_BERT\", dedup_and_sort, \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training Legal BERT norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_BERT\", norm_dedup_sort, \"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "print('-'*50)\n",
    "print('Training legal_longformer original_dataset')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_longformer\", original_dataset, \"original_dataset\")\n",
    "print('-'*50)\n",
    "print('Training legal_longformer dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_longformer\", dedup_and_sort, \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training legal_longformer norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_longformer\", norm_dedup_sort, \"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "## legal_Roberta on all three datasets \n",
    "\n",
    "# train_transformer_model(\"legal_Roberta\", original_dataset, \"Original\")\n",
    "print('-'*50)\n",
    "print('Training legal_Roberta original_dataset')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_Roberta\", original_dataset, \"original_dataset\")\n",
    "print('-'*50)\n",
    "print('Training legal_Roberta dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_Roberta\", dedup_and_sort, \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training legal_Roberta norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_Roberta\", norm_dedup_sort,\"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "\n",
    "# ----- Train TF-IDF + SVM -----\n",
    "def train_svm(dataset, dataset_label=\"\"):\n",
    "    print(f\"\\nðŸš€ Training TF-IDF + SVM on {dataset_label}...\")\n",
    "\n",
    "    train_texts = dataset[\"train\"][\"text\"]\n",
    "    train_labels = dataset[\"train\"][\"label\"]\n",
    "    val_texts = dataset[\"validation\"][\"text\"]\n",
    "    val_labels = dataset[\"validation\"][\"label\"]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    clf = LinearSVC(max_iter=epochs * 100)  # Emulating epoch-based behavior\n",
    "    clf.fit(X_train, train_labels)\n",
    "    preds = clf.predict(X_val)\n",
    "\n",
    "    model_name = f\"tfidf_svm ({dataset_label})\"\n",
    "    results[model_name] = {\n",
    "        \"Micro F1\": round(f1_score(val_labels, preds, average=\"micro\") * 100, 2),\n",
    "        \"Macro F1\": round(f1_score(val_labels, preds, average=\"macro\") * 100, 2),\n",
    "        \"Dataset\": dataset_label\n",
    "    }\n",
    "\n",
    "## SVM on all three datasets \n",
    "print('-'*50)\n",
    "print('Training SVM OG')\n",
    "print('-'*50)\n",
    "train_svm(original_dataset, \"Original\")\n",
    "print('-'*50)\n",
    "print('Training SVM dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_svm(dedup_and_sort,  \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training SVM norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_svm(norm_dedup_sort, \"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "# ----- Print Table -----\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Results Summary:\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# ----- Plot Chart -----\n",
    "results_df.plot(kind=\"bar\", figsize=(14, 8), ylim=(0, 100))\n",
    "plt.title(\"Model Comparison: Micro and Macro F1 Scores\")\n",
    "plt.ylabel(\"F1 Score (%)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# results_df.to_excel('/home/srmist5/victor/BERT_Optimize/new_final/revised_results_variant_1_performance.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamil",
   "language": "python",
   "name": "tamil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
