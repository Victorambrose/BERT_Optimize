{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook deals with SORTING | workflow (1) : Deduplicate --> Sort | workflow (1) : Normalize --> Deduplicate --> Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:49:38.205741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744197578.227212  822642 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744197578.233931  822642 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744197578.249081  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744197578.249102  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744197578.249105  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744197578.249106  822642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 16:49:38.254562: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5,6'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose three models , legal_BERT , legal_longformer , legal_Roberta\n",
    "## datasets :\n",
    "\n",
    "1) Original SCOTUS \n",
    "2) Dedup and sort \n",
    "3) Normalize Deduplicate and Sort the words bassed on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800d86e5b6cd44c1bd4dbab4ad3216f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d354ffb0d1f04c29b526f120d1d57508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/41.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b4b1c2c836401c80156cbf708c0d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8446535d604a82970951ba1d978ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367867ecc0f0445a9f6037f2b80e1acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25f3382c60340dc8e69d41363efb709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842b07954fb54b3c84ccd7644f485693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training Legal BERT original dataset\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_822642/2127134393.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 626/2500 02:57 < 08:52, 3.52 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.279800</td>\n",
       "      <td>2.004190</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.111831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.290700</td>\n",
       "      <td>1.266581</td>\n",
       "      <td>0.647857</td>\n",
       "      <td>0.378878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.984531</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.452076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>0.738571</td>\n",
       "      <td>0.604348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.843505</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.655031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_names = {\n",
    "    \"legal_BERT\": \"nlpaueb/legal-bert-base-uncased\",\n",
    "    \"legal_longformer\": \"lexlms/legal-longformer-base\",\n",
    "    \"legal_Roberta\":\"lexlms/legal-roberta-base\"\n",
    "}\n",
    "\n",
    "learning_rate = 3e-5\n",
    "epochs = 20\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "# ----- Load Dataset -----\n",
    "original_dataset=load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n",
    "dedup_and_sort = load_dataset(\"victorambrose11/scotus_deduplicate_sort\")\n",
    "norm_dedup_sort = load_dataset(\"victorambrose11/scotus_normalize_deduplicate_sort\")\n",
    "\n",
    "label_list = original_dataset[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# ----- Token Classification Metric -----\n",
    "def compute_f1(pred):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ----- Training Function -----\n",
    "def train_transformer_model(model_key, dataset, dataset_label):\n",
    "    model_checkpoint = model_names[model_key]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "    # Preprocessing\n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    encoded_dataset = dataset.map(preprocess, batched=True)\n",
    "    encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "    encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_key}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        seed=seed,\n",
    "        logging_dir=f\"./logs_{model_key}\",\n",
    "        logging_steps=50,\n",
    "        warmup_steps=500,\n",
    "        lr_scheduler_type=\"linear\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        compute_metrics=compute_f1,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"DEBUG: Evaluation Metrics: {metrics}\")\n",
    "    results[f\"{model_key} ({dataset_label})\"] = {\n",
    "        \"Micro F1\": round(metrics.get(\"eval_micro_f1\", 0.0) * 100, 2),\n",
    "        \"Macro F1\": round(metrics.get(\"eval_macro_f1\", 0.0) * 100, 2),\n",
    "        \"Dataset\": dataset_label\n",
    "    }\n",
    "# ----- Train LegalBERT and LegalLongformer -----\n",
    "# original_dataset\n",
    "# tfidf_srt_original_dataset \n",
    "# tfidf_srt_normalized_dataset\n",
    "\n",
    "## legal bert on all three datasets \n",
    "print('-'*50)\n",
    "print('Training Legal BERT original dataset')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_BERT\", original_dataset, \"Original\")\n",
    "print('-'*50)\n",
    "print('Training Legal BERT dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_BERT\", dedup_and_sort, \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training Legal BERT norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_BERT\", norm_dedup_sort, \"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "print('-'*50)\n",
    "print('Training legal_longformer original_dataset')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_longformer\", original_dataset, \"original_dataset\")\n",
    "print('-'*50)\n",
    "print('Training legal_longformer dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_longformer\", dedup_and_sort, \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training legal_longformer norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_longformer\", norm_dedup_sort, \"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "## legal_Roberta on all three datasets \n",
    "\n",
    "# train_transformer_model(\"legal_Roberta\", original_dataset, \"Original\")\n",
    "print('-'*50)\n",
    "print('Training legal_Roberta original_dataset')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_Roberta\", original_dataset, \"original_dataset\")\n",
    "print('-'*50)\n",
    "print('Training legal_Roberta dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_Roberta\", dedup_and_sort, \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training legal_Roberta norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_transformer_model(\"legal_Roberta\", norm_dedup_sort,\"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "\n",
    "# ----- Train TF-IDF + SVM -----\n",
    "def train_svm(dataset, dataset_label=\"\"):\n",
    "    print(f\"\\nðŸš€ Training TF-IDF + SVM on {dataset_label}...\")\n",
    "\n",
    "    train_texts = dataset[\"train\"][\"text\"]\n",
    "    train_labels = dataset[\"train\"][\"label\"]\n",
    "    val_texts = dataset[\"validation\"][\"text\"]\n",
    "    val_labels = dataset[\"validation\"][\"label\"]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=10000)\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    clf = LinearSVC(max_iter=epochs * 100)  # Emulating epoch-based behavior\n",
    "    clf.fit(X_train, train_labels)\n",
    "    preds = clf.predict(X_val)\n",
    "\n",
    "    model_name = f\"tfidf_svm ({dataset_label})\"\n",
    "    results[model_name] = {\n",
    "        \"Micro F1\": round(f1_score(val_labels, preds, average=\"micro\") * 100, 2),\n",
    "        \"Macro F1\": round(f1_score(val_labels, preds, average=\"macro\") * 100, 2),\n",
    "        \"Dataset\": dataset_label\n",
    "    }\n",
    "\n",
    "## SVM on all three datasets \n",
    "print('-'*50)\n",
    "print('Training SVM OG')\n",
    "print('-'*50)\n",
    "train_svm(original_dataset, \"Original\")\n",
    "print('-'*50)\n",
    "print('Training SVM dedup_and_sort')\n",
    "print('-'*50)\n",
    "train_svm(dedup_and_sort,  \"dedup_and_sort\")\n",
    "print('-'*50)\n",
    "print('Training SVM norm_dedup_sort')\n",
    "print('-'*50)\n",
    "train_svm(norm_dedup_sort, \"norm_dedup_sort\")\n",
    "\n",
    "\n",
    "# ----- Print Table -----\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Results Summary:\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# ----- Plot Chart -----\n",
    "results_df.plot(kind=\"bar\", figsize=(14, 8), ylim=(0, 100))\n",
    "plt.title(\"Model Comparison: Micro and Macro F1 Scores\")\n",
    "plt.ylabel(\"F1 Score (%)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# results_df.to_excel('/home/srmist5/victor/BERT_Optimize/new_final/revised_results_variant_1_performance.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamil",
   "language": "python",
   "name": "tamil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
