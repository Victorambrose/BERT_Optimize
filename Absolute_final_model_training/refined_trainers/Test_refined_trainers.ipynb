{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# ðŸ“’ Notebook: Legal Model Training (TFIDF-SRT)\n",
    "# ===========================================\n",
    "\n",
    "# ------- 1. Setup --------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5,6'\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments, set_seed\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Set seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# ------- 2. Configs --------\n",
    "model_names = {\n",
    "    \"legal_BERT\": \"nlpaueb/legal-bert-base-uncased\",\n",
    "    \"legal_longformer\": \"lexlms/legal-longformer-base\",\n",
    "    \"legal_Roberta\": \"lexlms/legal-roberta-base\"\n",
    "}\n",
    "\n",
    "learning_rates = [1e-5, 2e-5, 3e-5]\n",
    "dropout_rate = 0.1\n",
    "epochs_list = [3, 4]\n",
    "tfidf_bucket_sizes = [16, 32]\n",
    "\n",
    "# ------- 3. Load Datasets --------\n",
    "original_dataset = load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n",
    "dedup_and_sort = load_dataset(\"victorambrose11/scotus_deduplicate_sort\")\n",
    "norm_dedup_sort = load_dataset(\"victorambrose11/scotus_normalize_deduplicate_sort\")\n",
    "\n",
    "label_list = original_dataset[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# ------- 4. TFIDF-SRT-EMB Preprocessing --------\n",
    "def tfidf_score_to_bucket(score, num_buckets=32, min_val=0.0, max_val=10.0):\n",
    "    if score < min_val:\n",
    "        return 0\n",
    "    elif score >= max_val:\n",
    "        return num_buckets - 1\n",
    "    normalized = (score - min_val) / (max_val - min_val)\n",
    "    return int(normalized * (num_buckets - 1))\n",
    "\n",
    "def preprocess_tfidf_srt_emb(dataset, tokenizer, max_length=512, num_buckets=32):\n",
    "    tokenized_train = [\" \".join(tokenizer.tokenize(text)) for text in dataset['train']['text']]\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+')\n",
    "    tfidf_vectorizer.fit(tokenized_train)\n",
    "    idf_dict = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "\n",
    "    def process_split(split_name):\n",
    "        data = []\n",
    "        for text, label in zip(dataset[split_name]['text'], dataset[split_name]['label']):\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            seen = set()\n",
    "            unique_tokens = []\n",
    "            for t in tokens:\n",
    "                if t not in seen:\n",
    "                    unique_tokens.append(t)\n",
    "                    seen.add(t)\n",
    "            token_scores = {t: idf_dict.get(t, 0.0) for t in unique_tokens}\n",
    "            sorted_tokens = sorted(unique_tokens, key=lambda t: token_scores[t], reverse=True)[:max_length - 2]\n",
    "            tokens_final = [tokenizer.cls_token] + sorted_tokens + [tokenizer.sep_token]\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens_final)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            bucket_ids = [0] + [tfidf_score_to_bucket(token_scores.get(t, 0.0), num_buckets) for t in sorted_tokens] + [0]\n",
    "            pad_len = max_length - len(input_ids)\n",
    "            input_ids += [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            bucket_ids += [0] * pad_len\n",
    "            data.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label, \"tfidf_bucket_ids\": bucket_ids})\n",
    "        return Dataset.from_list(data)\n",
    "\n",
    "    return {\n",
    "        \"train\": process_split(\"train\"),\n",
    "        \"validation\": process_split(\"validation\")\n",
    "    }\n",
    "\n",
    "# ------- 5. Custom Model for TFIDF-SRT-EMB --------\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class TfidfSRTEMBLegalBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, bucket_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.base_model = BertModel.from_pretrained(model_name)\n",
    "        self.bucket_embedding = nn.Embedding(bucket_size, self.base_model.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tfidf_bucket_ids, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        bucket_embeds = self.bucket_embedding(tfidf_bucket_ids)\n",
    "        combined = token_embeddings + bucket_embeds\n",
    "        pooled = combined[:, 0]  # CLS token\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# ------- 6. Training Function & Grid Search --------\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def compute_f1(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(model_key, dataset, dataset_label, lr, epochs, is_tfidf_emb=False, bucket_size=32):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[model_key])\n",
    "\n",
    "    if is_tfidf_emb:\n",
    "        processed_dataset = preprocess_tfidf_srt_emb(dataset, tokenizer, num_buckets=bucket_size)\n",
    "        model = TfidfSRTEMBLegalBERT(model_name=model_names[model_key], num_labels=num_labels, bucket_size=bucket_size)\n",
    "\n",
    "        def collate_fn(batch):\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch]),\n",
    "                \"attention_mask\": torch.tensor([item[\"attention_mask\"] for item in batch]),\n",
    "                \"tfidf_bucket_ids\": torch.tensor([item[\"tfidf_bucket_ids\"] for item in batch]),\n",
    "                \"labels\": torch.tensor([item[\"labels\"] for item in batch])\n",
    "            }\n",
    "    else:\n",
    "        def tokenize_fn(examples):\n",
    "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        processed_dataset = dataset.map(tokenize_fn, batched=True)\n",
    "        processed_dataset = processed_dataset.rename_column(\"label\", \"labels\")\n",
    "        processed_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_names[model_key], num_labels=num_labels)\n",
    "        collate_fn = None\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_key}_{dataset_label}_{lr}_{epochs}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        logging_dir=f\"./logs_{model_key}_{dataset_label}\",\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=processed_dataset[\"train\"],\n",
    "        eval_dataset=processed_dataset[\"validation\"],\n",
    "        compute_metrics=compute_f1,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collate_fn\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()\n",
    "\n",
    "# ------- 7. Run Grid Search --------\n",
    "datasets_dict = {\n",
    "    \"original\": original_dataset,\n",
    "    \"dedup_sort\": dedup_and_sort,\n",
    "    \"norm_dedup_sort\": norm_dedup_sort\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_key in model_names:\n",
    "    for dataset_label, dataset in datasets_dict.items():\n",
    "        for lr, ep in itertools.product(learning_rates, epochs_list):\n",
    "            print(f\"\\nðŸš€ Training {model_key} on {dataset_label} | lr={lr}, epochs={ep}\")\n",
    "            metrics = train_and_evaluate(model_key, dataset, dataset_label, lr, ep)\n",
    "            results.append({\"Model\": model_key, \"Dataset\": dataset_label, \"lr\": lr, \"Epochs\": ep, **metrics})\n",
    "\n",
    "# TFIDF-SRT-EMB grid search\n",
    "for bucket_size in tfidf_bucket_sizes:\n",
    "    for lr, ep in itertools.product(learning_rates, epochs_list):\n",
    "        print(f\"\\nðŸ“˜ Training TFIDF-SRT-EMB (LegalBERT) | buckets={bucket_size}, lr={lr}, epochs={ep}\")\n",
    "        metrics = train_and_evaluate(\"legal_BERT\", original_dataset, f\"tfidf_emb_{bucket_size}\", lr, ep, is_tfidf_emb=True, bucket_size=bucket_size)\n",
    "        results.append({\"Model\": \"TFIDF-SRT-EMB\", \"Dataset\": \"original\", \"Buckets\": bucket_size, \"lr\": lr, \"Epochs\": ep, **metrics})\n",
    "\n",
    "# ------- 8. Results Table --------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"training_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… All training complete. Top results:\")\n",
    "print(results_df.sort_values(\"macro_f1\", ascending=False).head(10))\n",
    "\n",
    "# ------- 9. Plot --------\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df.groupby(\"Model\")[\"macro_f1\"].max().sort_values().plot(kind=\"barh\")\n",
    "plt.title(\"Max Macro F1 Score per Model\")\n",
    "plt.xlabel(\"Macro F1\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
