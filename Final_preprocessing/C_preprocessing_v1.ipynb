{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Preprocesses data for TFIDF-SRT-LegalBERT:\n",
    "1. Tokenizes documents\n",
    "2. Removes duplicate tokens\n",
    "3. Sorts remaining tokens by TF-IDF scores\n",
    "4. Truncates to 512 tokens if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "original_dataset=load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_tfidf_srt_legalbert(dataset, tokenizer_name=\"nlpaueb/legal-bert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with train/validation/test splits\n",
    "        tokenizer_name: Name of the tokenizer to use\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed dataset with new 'processed_input_ids' and 'processed_attention_mask' fields\n",
    "    \"\"\"\n",
    "    # Load LegalBERT tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    # First, we need to tokenize all documents to build the TF-IDF vocabulary\n",
    "    tokenized_texts = {split: [] for split in dataset.keys()}\n",
    "    \n",
    "    # Tokenize all texts and store them as lists of token strings\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Processing {split} set with TF-IDF sorting and deduplication...\")\n",
    "        \n",
    "        texts = dataset[split]['text']\n",
    "        labels = dataset[split]['labels'] if 'labels' in dataset[split] else [None] * len(texts)\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            # Tokenize text\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            \n",
    "            # Calculate TF for each token\n",
    "            token_counts = Counter(tokens)\n",
    "            total_tokens = len(tokens)\n",
    "            \n",
    "            # Calculate TF-IDF scores\n",
    "            tfidf_scores = {}\n",
    "            for token, count in token_counts.items():\n",
    "                tf = count / total_tokens\n",
    "                idf = token_idf.get(token, 1.0)\n",
    "                tfidf_scores[token] = tf * idf\n",
    "            \n",
    "            # Remove duplicates (keep only first occurrence)\n",
    "            seen = set()\n",
    "            unique_tokens = []\n",
    "            for token in tokens:\n",
    "                if token not in seen:\n",
    "                    unique_tokens.append(token)\n",
    "                    seen.add(token)\n",
    "            \n",
    "            # Sort tokens by TF-IDF scores (decreasing order)\n",
    "            sorted_tokens = sorted(unique_tokens, key=lambda t: tfidf_scores.get(t, 0.0), reverse=True)\n",
    "            \n",
    "            # Convert tokens to input IDs\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(sorted_tokens)\n",
    "            \n",
    "            # Add special tokens\n",
    "            input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n",
    "            \n",
    "            # Create attention mask based on TF-IDF scores\n",
    "            attention_mask = [1.0]  # For CLS token\n",
    "            for token in sorted_tokens:\n",
    "                attention_mask.append(min(1.0, tfidf_scores[token] / max(tfidf_scores.values())))\n",
    "            attention_mask.append(1.0)  # For SEP token\n",
    "            \n",
    "            # Truncate or pad sequences\n",
    "            if len(input_ids) > 512:\n",
    "                input_ids = input_ids[:511] + [tokenizer.sep_token_id]\n",
    "                attention_mask = attention_mask[:512]\n",
    "            else:\n",
    "                padding_length = 512 - len(input_ids)\n",
    "                input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0.0] * padding_length\n",
    "            \n",
    "            processed_dataset[split]['input_ids'].append(input_ids)\n",
    "            processed_dataset[split]['attention_mask'].append(attention_mask)\n",
    "            processed_dataset[split]['labels'].append(label)\n",
    "    \n",
    "    # Convert to DataFrame format\n",
    "    result_dataset = {}\n",
    "    for split in dataset.keys():\n",
    "        result_dataset[split] = pd.DataFrame(processed_dataset[split])\n",
    "    \n",
    "    return result_dataset\n",
    "\n",
    "TF_IDF_SRT = preprocess_tfidf_srt_legalbert(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4330 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train set...\n",
      "Tokenizing test set...\n",
      "Tokenizing validation set...\n",
      "Fitting TF-IDF vectorizer on training data...\n",
      "Processing train set with TF-IDF sorting and deduplication...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set with TF-IDF sorting and deduplication...\n",
      "Processing validation set with TF-IDF sorting and deduplication...\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_SRT = preprocess_tfidf_srt_legalbert(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TF_IDF_SRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_SRT['train']['labels']=original_dataset['train']['label']\n",
    "TF_IDF_SRT['test']['labels']=original_dataset['test']['label']\n",
    "TF_IDF_SRT['validation']['labels']=original_dataset['validation']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for i in range(len(TF_IDF_SRT['train'])):\n",
    "    current_mask=str(TF_IDF_SRT['train']['attention_mask'][i])\n",
    "    if  current_mask not in d:\n",
    "        d[current_mask]=1\n",
    "    else:\n",
    "        d[current_mask]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [101, 207, 117, 115, 3710, 13169, 2282, 210, 2...\n",
       "1    [101, 117, 207, 115, 210, 14733, 12067, 211, 1...\n",
       "2    [101, 117, 115, 207, 2479, 210, 26432, 4313, 8...\n",
       "3    [101, 117, 115, 207, 4962, 210, 11955, 211, 48...\n",
       "Name: input_ids, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_SRT['train']['input_ids'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4330 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train set...\n",
      "Tokenizing test set...\n",
      "Tokenizing validation set...\n",
      "Fitting TF-IDF vectorizer on training data...\n",
      "Processing train set with TF-IDF sorting and deduplication...\n",
      "Processing test set with TF-IDF sorting and deduplication...\n",
      "Processing validation set with TF-IDF sorting and deduplication...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_tfidf_srt_legalbert(dataset, tokenizer_name=\"nlpaueb/legal-bert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Preprocesses data for TFIDF-SRT-LegalBERT:\n",
    "    1. Tokenizes documents\n",
    "    2. Removes duplicate tokens\n",
    "    3. Sorts remaining tokens by TF-IDF scores\n",
    "    4. Truncates to 512 tokens if needed\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with train/validation/test splits\n",
    "        tokenizer_name: Name of the tokenizer to use\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed dataset with new 'input_ids' and 'attention_mask' fields\n",
    "    \"\"\"\n",
    "    # Load LegalBERT tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    # First, we need to tokenize all documents to build the TF-IDF vocabulary\n",
    "    tokenized_texts = {split: [] for split in dataset.keys()}\n",
    "    \n",
    "    # Tokenize all texts and store them as lists of token strings\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Tokenizing {split} set...\")\n",
    "        \n",
    "        # Assuming the input text is in 'text' field - adjust as needed\n",
    "        texts = dataset[split]['text']\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize without adding special tokens yet\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            tokenized_texts[split].append(tokens)\n",
    "    \n",
    "    # Combine all training tokens for fitting TF-IDF\n",
    "    all_train_tokens = [' '.join(tokens) for tokens in tokenized_texts['train']]\n",
    "    \n",
    "    # Fit TF-IDF vectorizer on training data\n",
    "    print(\"Fitting TF-IDF vectorizer on training data...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+')\n",
    "    tfidf_vectorizer.fit(all_train_tokens)\n",
    "    \n",
    "    # Create vocabulary dictionary that maps tokens to their IDF values\n",
    "    vocabulary = tfidf_vectorizer.vocabulary_\n",
    "    idfs = tfidf_vectorizer.idf_\n",
    "    token_idf = {token: idfs[idx] for token, idx in vocabulary.items()}\n",
    "    \n",
    "    # Process each split\n",
    "    processed_dataset = {split: {'input_ids': [], 'attention_mask': [], 'labels': []} for split in dataset.keys()}\n",
    "    \n",
    "    for split in dataset.keys():\n",
    "        print(f\"Processing {split} set with TF-IDF sorting and deduplication...\")\n",
    "        \n",
    "        # Get all texts and labels for this split\n",
    "        texts = dataset[split]['text']\n",
    "        labels = dataset[split]['labels'] if 'labels' in dataset[split] else [None] * len(texts)\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            # Tokenize text\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            \n",
    "            # Calculate TF for each token\n",
    "            token_counts = Counter(tokens)\n",
    "            total_tokens = len(tokens)\n",
    "            \n",
    "            # Calculate TF-IDF scores\n",
    "            tfidf_scores = {}\n",
    "            for token, count in token_counts.items():\n",
    "                tf = count / total_tokens\n",
    "                # Use default IDF value of 1.0 if token not in training vocabulary\n",
    "                idf = token_idf.get(token, 1.0)\n",
    "                tfidf_scores[token] = tf * idf\n",
    "            \n",
    "            # Remove duplicates (keep only first occurrence)\n",
    "            seen = set()\n",
    "            unique_tokens = []\n",
    "            for token in tokens:\n",
    "                if token not in seen:\n",
    "                    unique_tokens.append(token)\n",
    "                    seen.add(token)\n",
    "            \n",
    "            # Sort tokens by TF-IDF scores (decreasing order)\n",
    "            sorted_tokens = sorted(unique_tokens, key=lambda t: tfidf_scores.get(t, 0.0), reverse=True)\n",
    "            \n",
    "            # Convert tokens to input IDs\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(sorted_tokens)\n",
    "            \n",
    "            # Add special tokens\n",
    "            input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id]\n",
    "            \n",
    "            # Create attention mask based on TF-IDF scores\n",
    "            attention_mask = [1.0]  # For CLS token\n",
    "            for token in sorted_tokens:\n",
    "                attention_mask.append(min(1.0, tfidf_scores[token] / max(tfidf_scores.values())))\n",
    "            attention_mask.append(1.0)  # For SEP token\n",
    "            \n",
    "            # Truncate or pad sequences\n",
    "            if len(input_ids) > 512:\n",
    "                input_ids = input_ids[:511] + [tokenizer.sep_token_id]\n",
    "                attention_mask = attention_mask[:512]\n",
    "            else:\n",
    "                padding_length = 512 - len(input_ids)\n",
    "                input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "                attention_mask = attention_mask + [0.0] * padding_length\n",
    "            \n",
    "            processed_dataset[split]['input_ids'].append(input_ids)\n",
    "            processed_dataset[split]['attention_mask'].append(attention_mask)\n",
    "            processed_dataset[split]['labels'].append(label)\n",
    "    \n",
    "    # Convert to DataFrame format\n",
    "    result_dataset = {}\n",
    "    for split in dataset.keys():\n",
    "        result_dataset[split] = pd.DataFrame(processed_dataset[split])\n",
    "    \n",
    "    return result_dataset\n",
    "\n",
    "# Example of how to use the function:\n",
    "# preprocessed_data = preprocess_tfidf_srt_legalbert(dataset)\n",
    "TF_IDF_SRT_V2 = preprocess_tfidf_srt_legalbert(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Example: if tfidf_srt_processed is a dict of lists or pandas DataFrames\n",
    "# Convert it to DatasetDict\n",
    "if isinstance(TF_IDF_SRT_V2, dict):\n",
    "    TF_IDF_SRT_V2 = DatasetDict({\n",
    "        split: Dataset.from_pandas(data) if not isinstance(data, Dataset) else data\n",
    "        for split, data in TF_IDF_SRT_V2.items()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 34.50ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 37.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 45.46ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/victorambrose11/TF_IDF_SRT_V2/commit/2102ee1bf685a1498d5fdbc3535545d99b4b776f', commit_message='Upload dataset', commit_description='', oid='2102ee1bf685a1498d5fdbc3535545d99b4b776f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/victorambrose11/TF_IDF_SRT_V2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='victorambrose11/TF_IDF_SRT_V2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_SRT_V2.push_to_hub(\"victorambrose11/TF_IDF_SRT_V2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
