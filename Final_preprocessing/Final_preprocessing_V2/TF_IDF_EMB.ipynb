{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Preprocesses data for TFIDF-EMB-LegalBERT:\n",
    "1. Tokenizes documents normally (maintains word order)\n",
    "2. Calculates TF-IDF scores for all tokens\n",
    "3. Bucketizes TF-IDF scores\n",
    "4. Adds bucket IDs for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "def tfidf_score_to_bucket(score, num_buckets=32, min_val=0.0, max_val=10.0):\n",
    "    \"\"\"Bucketize a continuous TF-IDF score into a discrete bin.\"\"\"\n",
    "    if score < min_val:\n",
    "        return 0\n",
    "    elif score >= max_val:\n",
    "        return num_buckets - 1\n",
    "    normalized = (score - min_val) / (max_val - min_val)\n",
    "    return int(normalized * (num_buckets - 1))\n",
    "\n",
    "def preprocess_tfidf_srt_emb(\n",
    "    dataset: DatasetDict,\n",
    "    tokenizer_name=\"nlpaueb/legal-bert-base-uncased\",\n",
    "    max_length=512,\n",
    "    num_buckets=32\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Step 1: Fit TF-IDF on training set (tokenized)\n",
    "    print(\"Tokenizing training set for TF-IDF fitting...\")\n",
    "    tokenized_train = [\" \".join(tokenizer.tokenize(text)) for text in dataset['train']['text']]\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+')\n",
    "    tfidf_vectorizer.fit(tokenized_train)\n",
    "\n",
    "    idf_dict = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "\n",
    "    processed_data = {}\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        tfidf_buckets_list = []\n",
    "        labels = dataset[split]['label'] if 'label' in dataset[split].features else [None] * len(dataset[split])\n",
    "\n",
    "        for text in dataset[split]['text']:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            # Deduplicate (keep first occurrence)\n",
    "            seen = set()\n",
    "            unique_tokens = []\n",
    "            for token in tokens:\n",
    "                if token not in seen:\n",
    "                    unique_tokens.append(token)\n",
    "                    seen.add(token)\n",
    "\n",
    "            # Score tokens by IDF only (TF not used)\n",
    "            token_scores = {t: idf_dict.get(t, 0.0) for t in unique_tokens}\n",
    "            sorted_tokens = sorted(unique_tokens, key=lambda t: token_scores[t], reverse=True)\n",
    "\n",
    "            # Truncate to fit CLS/SEP\n",
    "            sorted_tokens = sorted_tokens[:max_length - 2]\n",
    "            tokens_final = [tokenizer.cls_token] + sorted_tokens + [tokenizer.sep_token]\n",
    "\n",
    "            # Convert to IDs\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens_final)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Compute bucket IDs\n",
    "            sorted_scores = [token_scores.get(t, 0.0) for t in sorted_tokens]\n",
    "            bucket_ids = [0] + [tfidf_score_to_bucket(s, num_buckets) for s in sorted_scores] + [0]  # CLS/SEP bucket=0\n",
    "\n",
    "            # Pad if needed\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "            bucket_ids += [0] * padding_length\n",
    "\n",
    "            input_ids_list.append(input_ids)\n",
    "            attention_mask_list.append(attention_mask)\n",
    "            tfidf_buckets_list.append(bucket_ids)\n",
    "\n",
    "        processed_data[split] = pd.DataFrame({\n",
    "            \"input_ids\": input_ids_list,\n",
    "            \"attention_mask\": attention_mask_list,\n",
    "            \"tfidf_bucket_ids\": tfidf_buckets_list,\n",
    "            \"label\": labels\n",
    "        })\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"victorambrose11/lex_glue_original_TFIDF-SRT\")\n",
    "TF_IDF_EMB = preprocess_tfidf_srt_emb(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
