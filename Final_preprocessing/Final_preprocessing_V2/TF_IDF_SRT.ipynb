{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF_IDF_SRT\n",
    "\n",
    "- Uses the LegalBERT tokenizer\n",
    "- Removes duplicate subword tokens\n",
    "- Computes TF-IDF scores from the training split\n",
    "- Sorts tokens by descending TF-IDF score\n",
    "- Truncates to 512 tokens (plus [CLS] and [SEP])\n",
    "- Returns padded input_ids and attention_mask (binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "def tfidf_srt_preprocess(dataset: DatasetDict, tokenizer_name=\"nlpaueb/legal-bert-base-uncased\", max_length=512):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Step 1: Tokenize train texts for TF-IDF vocabulary\n",
    "    print(\"Tokenizing and preparing training corpus for TF-IDF...\")\n",
    "    tokenized_train = [\" \".join(tokenizer.tokenize(text)) for text in dataset['train']['text']]\n",
    "\n",
    "    # Step 2: Fit TF-IDF vectorizer on tokenized training data\n",
    "    print(\"Fitting TF-IDF vectorizer...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+')\n",
    "    tfidf_vectorizer.fit(tokenized_train)\n",
    "    idf_dict = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "\n",
    "    # Step 3: Preprocess each split using TFIDF-SRT\n",
    "    processed_data = {}\n",
    "    for split in dataset.keys():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        labels = dataset[split]['label'] if 'label' in dataset[split].features else [None] * len(dataset[split])\n",
    "\n",
    "        for text in dataset[split]['text']:\n",
    "            # Tokenize\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            # Deduplicate tokens (keep first occurrence)\n",
    "            seen = set()\n",
    "            unique_tokens = []\n",
    "            for t in tokens:\n",
    "                if t not in seen:\n",
    "                    unique_tokens.append(t)\n",
    "                    seen.add(t)\n",
    "\n",
    "            # Score tokens by IDF (TF not needed as per paper)\n",
    "            sorted_tokens = sorted(unique_tokens, key=lambda t: idf_dict.get(t, 0), reverse=True)\n",
    "\n",
    "            # Truncate tokens to max usable length (leave space for CLS and SEP)\n",
    "            sorted_tokens = sorted_tokens[:max_length - 2]\n",
    "\n",
    "            # Add special tokens\n",
    "            tokens_final = [tokenizer.cls_token] + sorted_tokens + [tokenizer.sep_token]\n",
    "\n",
    "            # Convert to input_ids\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens_final)\n",
    "\n",
    "            # Pad if needed\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask = [1] * len(tokens_final) + [0] * padding_length\n",
    "\n",
    "            input_ids_list.append(input_ids)\n",
    "            attention_mask_list.append(attention_mask)\n",
    "\n",
    "        processed_data[split] = pd.DataFrame({\n",
    "            \"input_ids\": input_ids_list,\n",
    "            \"attention_mask\": attention_mask_list,\n",
    "            \"label\": labels\n",
    "        })\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and preparing training corpus for TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4330 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer...\n",
      "Processing split: train\n",
      "Processing split: test\n",
      "Processing split: validation\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n",
    "tfidf_srt_processed = tfidf_srt_preprocess(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Example: if tfidf_srt_processed is a dict of lists or pandas DataFrames\n",
    "# Convert it to DatasetDict\n",
    "if isinstance(tfidf_srt_processed, dict):\n",
    "    tfidf_srt_processed = DatasetDict({\n",
    "        split: Dataset.from_pandas(data) if not isinstance(data, Dataset) else data\n",
    "        for split, data in tfidf_srt_processed.items()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 52.93ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 74.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 79.88ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/victorambrose11/tfidf_srt_processed/commit/8e823dbb918ac5eddbe421047fc2e68ac6038edc', commit_message='Upload dataset', commit_description='', oid='8e823dbb918ac5eddbe421047fc2e68ac6038edc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/victorambrose11/tfidf_srt_processed', endpoint='https://huggingface.co', repo_type='dataset', repo_id='victorambrose11/tfidf_srt_processed'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_srt_processed.push_to_hub(\"victorambrose11/tfidf_srt_processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
