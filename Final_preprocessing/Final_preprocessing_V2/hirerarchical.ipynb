{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Preprocesses data for Hierarchical LegalBERT:\n",
    "1. Splits documents into paragraphs\n",
    "2. Tokenizes each paragraph separately\n",
    "3. Creates a structure suitable for hierarchical processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "original_dataset=load_dataset(\"coastalcph/lex_glue\", \"scotus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_document(text, label, tokenizer, max_paragraphs, max_paragraph_length):\n",
    "    # Split text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    paragraphs = [p for p in paragraphs if p.strip()]\n",
    "    \n",
    "    # Limit the number of paragraphs if necessary\n",
    "    if len(paragraphs) > max_paragraphs:\n",
    "        paragraphs = paragraphs[:max_paragraphs]\n",
    "    \n",
    "    actual_paragraph_count = len(paragraphs)\n",
    "    \n",
    "    # Tokenize each paragraph\n",
    "    paragraph_encodings = [\n",
    "        tokenizer.encode_plus(\n",
    "            paragraph,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_paragraph_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        ) for paragraph in paragraphs\n",
    "    ]\n",
    "    \n",
    "    paragraph_input_ids = [encoding['input_ids'] for encoding in paragraph_encodings]\n",
    "    paragraph_attention_masks = [encoding['attention_mask'] for encoding in paragraph_encodings]\n",
    "    \n",
    "    # Pad to max_paragraphs if necessary\n",
    "    if len(paragraph_input_ids) < max_paragraphs:\n",
    "        pad_input_ids = [tokenizer.cls_token_id] + [tokenizer.pad_token_id] * (max_paragraph_length - 2) + [tokenizer.sep_token_id]\n",
    "        pad_attention_mask = [1, 0, 1] + [0] * (max_paragraph_length - 3)\n",
    "        \n",
    "        paragraph_input_ids.extend([pad_input_ids] * (max_paragraphs - len(paragraph_input_ids)))\n",
    "        paragraph_attention_masks.extend([pad_attention_mask] * (max_paragraphs - len(paragraph_attention_masks)))\n",
    "    \n",
    "    return paragraph_input_ids, paragraph_attention_masks, actual_paragraph_count, label\n",
    "\n",
    "def preprocess_hierarchical_legalbert(dataset, tokenizer_name=\"nlpaueb/legal-bert-base-uncased\", \n",
    "                                     max_paragraphs=20, max_paragraph_length=512, max_workers=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    processed_dataset = {split: {} for split in dataset.keys()}\n",
    "    \n",
    "    for split in dataset.keys():\n",
    "        print(f\"Processing {split} set for hierarchical encoding...\")\n",
    "        \n",
    "        texts = dataset[split]['text']\n",
    "        labels = dataset[split]['labels'] if 'labels' in dataset[split] else [None] * len(texts)\n",
    "        \n",
    "        doc_paragraph_input_ids = []\n",
    "        doc_paragraph_attention_masks = []\n",
    "        doc_paragraph_counts = []\n",
    "        doc_labels = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_doc = {executor.submit(process_document, text, label, tokenizer, max_paragraphs, max_paragraph_length): (text, label) \n",
    "                             for text, label in zip(texts, labels)}\n",
    "            \n",
    "            for future in as_completed(future_to_doc):\n",
    "                paragraph_input_ids, paragraph_attention_masks, actual_paragraph_count, label = future.result()\n",
    "                doc_paragraph_input_ids.append(paragraph_input_ids)\n",
    "                doc_paragraph_attention_masks.append(paragraph_attention_masks)\n",
    "                doc_paragraph_counts.append(actual_paragraph_count)\n",
    "                doc_labels.append(label)\n",
    "        \n",
    "        processed_dataset[split]['paragraph_input_ids'] = doc_paragraph_input_ids\n",
    "        processed_dataset[split]['paragraph_attention_masks'] = doc_paragraph_attention_masks\n",
    "        processed_dataset[split]['paragraph_counts'] = doc_paragraph_counts\n",
    "        processed_dataset[split]['labels'] = doc_labels\n",
    "    \n",
    "    result_dataset = {split: pd.DataFrame(data) for split, data in processed_dataset.items()}\n",
    "    \n",
    "    return result_dataset\n",
    "\n",
    "# Example usage:\n",
    "# preprocessed_data = preprocess_hierarchical_legalbert(dataset, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train set for hierarchical encoding...\n",
      "Processing test set for hierarchical encoding...\n",
      "Processing validation set for hierarchical encoding...\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = preprocess_hierarchical_legalbert(original_dataset, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Example: if tfidf_srt_processed is a dict of lists or pandas DataFrames\n",
    "# Convert it to DatasetDict\n",
    "if isinstance(preprocessed_data, dict):\n",
    "    preprocessed_data = DatasetDict({\n",
    "        split: Dataset.from_pandas(data) if not isinstance(data, Dataset) else data\n",
    "        for split, data in preprocessed_data.items()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  3.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  3.35ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:09<00:00,  4.56s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  3.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.25s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  4.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/victorambrose11/TF_IDF_EMB_Hirearchy/commit/b2afbee1b8f5d9dc4d3d72f0e148c095904e84a4', commit_message='Upload dataset', commit_description='', oid='b2afbee1b8f5d9dc4d3d72f0e148c095904e84a4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/victorambrose11/TF_IDF_EMB_Hirearchy', endpoint='https://huggingface.co', repo_type='dataset', repo_id='victorambrose11/TF_IDF_EMB_Hirearchy'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.push_to_hub(\"victorambrose11/TF_IDF_EMB_Hirearchy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
