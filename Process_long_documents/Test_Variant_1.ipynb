{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant 1: TFIDF-SRT-LegalBERT\n",
    "#### Overview:\n",
    "• The input document is first tokenized using the LegalBERT tokenizer.\n",
    "• Duplicate tokens are removed (while preserving only the first occurrence).\n",
    "• The remaining tokens are sorted in descending order by their TF-IDF score (precomputed on a training corpus).\n",
    "• The resulting ordered token string is re-tokenized (if needed) and fed into LegalBERT for classification.\n",
    "\n",
    "\n",
    "## Explanation Variant 1:\n",
    "\n",
    "• The TF-IDF vectorizer builds a dictionary of sub-word tokens mapped to their inverse document frequency (IDF) values.\n",
    "• The preprocess_document_bow function deduplicates tokens from each document and sorts them by their corresponding TF-IDF score.\n",
    "• The resulting ordered token string is then tokenized (again) to produce input IDs suitable for LegalBERT.\n",
    "\n",
    "Finally, these inputs are fed into the model for classification.\n",
    "\n",
    "• This variant does not modify the internal architecture of LegalBERT; it only changes the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"victorambrose11/normalized_scotus\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of documents in training dataset is 37956\n",
      "The lengthy document in the dataset contains 584365 number of tokens\n"
     ]
    }
   ],
   "source": [
    "highest=0\n",
    "total_length=0\n",
    "for idx in range(len(dataset['train'])):\n",
    "    total_length+=len(dataset['train'][idx]['text'])\n",
    "    if len(dataset['train'][idx]['text']) > highest:\n",
    "        highest=len(dataset['train'][idx]['text'])\n",
    "print (f'The average length of documents in training dataset is {round(total_length/len(dataset['train']))}\\nThe lengthy document in the dataset contains {highest} number of tokens')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Choose the LegalBERT model name (example)\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example training documents (in practice, you would use your full training corpus)\n",
    "documents = [\n",
    "    \"Legal document text example number one.\",\n",
    "    \"This is another long legal document for classification.\"\n",
    "]\n",
    "\n",
    "# ----- Preprocessing Step: Compute TF-IDF Scores -----\n",
    "# Using scikit-learn's TfidfVectorizer to compute TF-IDF on the raw documents.\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize, lowercase=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary mapping each token (sub-word) to its IDF score.\n",
    "idf_dict = {token: idf for token, idf in zip(feature_names, tfidf_vectorizer.idf_)}\n",
    "\n",
    "# ----- Preprocessing Function for TFIDF-SRT-LegalBERT -----\n",
    "def preprocess_document_bow(doc, tokenizer, idf_dict, max_length=512):\n",
    "    # Tokenize document using the LegalBERT tokenizer.\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # Deduplicate tokens: keeping only the first occurrence.\n",
    "    unique_tokens = list(dict.fromkeys(tokens))\n",
    "    \n",
    "    # For each token, fetch its TF-IDF score; use 0 if not found.\n",
    "    token_scores = [(token, idf_dict.get(token, 0)) for token in unique_tokens]\n",
    "    \n",
    "    # Sort tokens by TF-IDF score in descending order.\n",
    "    token_scores_sorted = sorted(token_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract tokens (and then truncate if necessary).\n",
    "    ordered_tokens = [token for token, score in token_scores_sorted][:max_length]\n",
    "    \n",
    "    # Join tokens into a single string for re-tokenization by LegalBERT (alternatively, convert directly to input IDs).\n",
    "    processed_text = \" \".join(ordered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Example: Preprocess documents using Variant 1 strategy.\n",
    "processed_docs_variant1 = [preprocess_document_bow(doc, tokenizer, idf_dict) for doc in documents]\n",
    "\n",
    "# Tokenize the preprocessed documents to obtain model inputs.\n",
    "inputs_variant1 = tokenizer(processed_docs_variant1, padding=True, truncation=True, \n",
    "                            max_length=512, return_tensors=\"pt\")\n",
    "outputs_variant1 = model(**inputs_variant1)\n",
    "loss_variant1 = outputs_variant1.loss\n",
    "logits_variant1 = outputs_variant1.logits\n",
    "\n",
    "print(\"Variant 1 logits:\", logits_variant1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
