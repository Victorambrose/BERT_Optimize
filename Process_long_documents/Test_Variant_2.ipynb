{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant 2: TFIDF-SRT-EMB-LegalBERT\n",
    "#### Overview:\n",
    "• This variant follows the same deduplication and TF-IDF-based reordering as Variant 1.\n",
    "• In addition, the model includes a TF-IDF embedding layer.\n",
    "• For each token in the input, you compute (or look up) its TF-IDF score, convert that into a “bucket” (or discretized index) based on predetermined bins, and then use a learnable embedding to map that bucket to a vector.\n",
    "• This additional embedding is then added to the regular token embedding.\n",
    "\n",
    "## Explanation Variant 2:\n",
    "\n",
    "##### Preprocessing:\n",
    "– We use a nearly identical preprocessing function as in Variant 1. In addition to returning the processed text, it also returns a list of the TF-IDF scores for the ordered tokens.\n",
    "– A helper function, bucketize_tfidf_score, maps each continuous TF-IDF score into a discrete bucket (the number of buckets is a hyperparameter, e.g., 32).\n",
    "\n",
    "##### TFIDF_EMB_LegalBERT Model:\n",
    "– The model wraps a standard LegalBERT and adds a learnable embedding layer (tfidf_embedding) that maps bucket indices to embedding vectors.\n",
    "– In the forward pass, the standard token embeddings (obtained from LegalBERT’s embedding layer) are combined with the projected TF-IDF embeddings (after passing through a linear projection to align dimensions).\n",
    "– The combined embeddings are then passed to LegalBERT using the inputs_embeds parameter.\n",
    "\n",
    "##### Data Preparation:\n",
    "– After preprocessing, we tokenize the processed text and simultaneously prepare the corresponding TF-IDF bucket IDs for every token.\n",
    "– These bucket IDs are then provided to the model along with the usual inputs.\n",
    "\n",
    "##### Training / Inference:\n",
    "– With this modified model, you can fine-tune on your classification task and compare the performance and efficiency to the baseline and to Variant 1.\n",
    "\n",
    "\n",
    "Below is an example implementation. In this example, we override the forward pass to add the extra TF-IDF embedding to the standard token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Assume we reuse the same model name and tokenizer as before.\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the base LegalBERT model.\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define a new model class for TFIDF-SRT-EMB-LegalBERT.\n",
    "class TFIDF_EMB_LegalBERT(nn.Module):\n",
    "    def __init__(self, base_model, num_buckets=32, embedding_dim=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_model: The pre-trained LegalBERT model.\n",
    "            num_buckets: Number of TF-IDF buckets to discretize the continuous scores.\n",
    "            embedding_dim: Dimension for the TF-IDF embeddings.\n",
    "        \"\"\"\n",
    "        super(TFIDF_EMB_LegalBERT, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        # Define an extra embedding layer for TF-IDF buckets.\n",
    "        self.tfidf_embedding = nn.Embedding(num_buckets, embedding_dim)\n",
    "        self.num_buckets = num_buckets\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # A simple projection to match base model's hidden size if needed.\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.proj = nn.Linear(embedding_dim, hidden_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, tfidf_bucket_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Tensor of token ids from the tokenizer.\n",
    "            attention_mask: Attention mask for padding tokens.\n",
    "            tfidf_bucket_ids: Tensor of the same shape as input_ids,\n",
    "                              indicating the bucket index for each token's TF-IDF score.\n",
    "        \"\"\"\n",
    "        # Get the standard token embeddings from the base model.\n",
    "        # Note: You can access the embeddings via base_model.bert.embeddings.word_embeddings for BERT.\n",
    "        token_embeddings = self.base_model.bert.embeddings.word_embeddings(input_ids)\n",
    "        \n",
    "        # Get the TF-IDF embeddings.\n",
    "        bucket_embeddings = self.tfidf_embedding(tfidf_bucket_ids)\n",
    "        # Project the TF-IDF embeddings into the same space as token embeddings.\n",
    "        bucket_embeddings_proj = self.proj(bucket_embeddings)\n",
    "        \n",
    "        # Combine the embeddings (e.g., add them).\n",
    "        combined_embeddings = token_embeddings + bucket_embeddings_proj\n",
    "        \n",
    "        # Now, call the rest of the model. We need to supply these combined embeddings.\n",
    "        # One way is to feed them to the encoder. Many Hugging Face models allow you to override embeddings via the \"inputs_embeds\" parameter.\n",
    "        outputs = self.base_model(\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=combined_embeddings,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "# For simplicity, assume we have a helper function to bucketize TF-IDF scores.\n",
    "def bucketize_tfidf_score(score, num_buckets, min_score=0.0, max_score=10.0):\n",
    "    \"\"\"\n",
    "    Map a continuous TF-IDF score to a discrete bucket.\n",
    "    You can adjust min_score and max_score based on your corpus statistics.\n",
    "    \"\"\"\n",
    "    # Normalize score to [0, 1]\n",
    "    norm_score = (score - min_score) / (max_score - min_score)\n",
    "    norm_score = max(0.0, min(1.0, norm_score))\n",
    "    bucket = int(norm_score * (num_buckets - 1))\n",
    "    return bucket\n",
    "\n",
    "# Using the same preprocess function as Variant 1 to deduplicate and reorder tokens.\n",
    "def preprocess_document_bow_variant(doc, tokenizer, idf_dict, max_length=512):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    unique_tokens = list(dict.fromkeys(tokens))\n",
    "    token_scores = [(token, idf_dict.get(token, 0)) for token in unique_tokens]\n",
    "    token_scores_sorted = sorted(token_scores, key=lambda x: x[1], reverse=True)\n",
    "    ordered_tokens = [token for token, score in token_scores_sorted][:max_length]\n",
    "    processed_text = \" \".join(ordered_tokens)\n",
    "    # Also return the corresponding TF-IDF scores for bucketization.\n",
    "    ordered_scores = [score for token, score in token_scores_sorted][:max_length]\n",
    "    return processed_text, ordered_scores\n",
    "\n",
    "# Assume we already computed the idf_dict as in Variant 1.\n",
    "# Preprocess the documents and compute the bucket IDs per token.\n",
    "processed_docs_variant2 = []\n",
    "tfidf_buckets_list = []  # List of lists for bucket ids\n",
    "for doc in documents:\n",
    "    proc_text, scores = preprocess_document_bow_variant(doc, tokenizer, idf_dict)\n",
    "    processed_docs_variant2.append(proc_text)\n",
    "    # Compute bucket indices for each token based on the TF-IDF score.\n",
    "    bucket_ids = [bucketize_tfidf_score(score, num_buckets=32) for score in scores]\n",
    "    tfidf_buckets_list.append(bucket_ids)\n",
    "\n",
    "# Tokenize the preprocessed documents.\n",
    "encoded_inputs = tokenizer(processed_docs_variant2, padding=True, truncation=True,\n",
    "                           max_length=512, return_tensors=\"pt\")\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "attention_mask = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "# For demonstration, create a tensor for tfidf_bucket_ids that aligns with input_ids.\n",
    "# Here we assume the tokenization of the processed text gives the same order as the list of bucket ids.\n",
    "# In a more careful implementation, you would map each token back to its bucket.\n",
    "# For simplicity, we pad/truncate the bucket ids list to match the token length.\n",
    "max_len = input_ids.size(1)\n",
    "tfidf_bucket_ids = []\n",
    "for bucket_ids in tfidf_buckets_list:\n",
    "    bucket_ids = bucket_ids[:max_len]\n",
    "    # Pad with 0s if necessary\n",
    "    if len(bucket_ids) < max_len:\n",
    "        bucket_ids.extend([0] * (max_len - len(bucket_ids)))\n",
    "    tfidf_bucket_ids.append(bucket_ids)\n",
    "tfidf_bucket_ids = torch.tensor(tfidf_bucket_ids)\n",
    "\n",
    "# Create an instance of the new model variant.\n",
    "model_variant2 = TFIDF_EMB_LegalBERT(base_model, num_buckets=32, embedding_dim=50)\n",
    "\n",
    "# Forward pass using variant 2.\n",
    "outputs_variant2 = model_variant2(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                  tfidf_bucket_ids=tfidf_bucket_ids)\n",
    "loss_variant2 = outputs_variant2.loss\n",
    "logits_variant2 = outputs_variant2.logits\n",
    "\n",
    "print(\"Variant 2 logits:\", logits_variant2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
