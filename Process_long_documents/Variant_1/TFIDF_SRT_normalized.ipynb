{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant 1.2 : TFIDF-SRT-LegalBERT ( Normalized )\n",
    "#### Overview:\n",
    "• The input document is first tokenized using the LegalBERT tokenizer.\n",
    "• Duplicate tokens are removed (while preserving only the first occurrence).\n",
    "• The remaining tokens are sorted in descending order by their TF-IDF score (precomputed on a training corpus).\n",
    "• The resulting ordered token string is re-tokenized (if needed) and fed into LegalBERT for classification.\n",
    "\n",
    "\n",
    "## Explanation Variant 1:\n",
    "\n",
    "• The TF-IDF vectorizer builds a dictionary of sub-word tokens mapped to their inverse document frequency (IDF) values.\n",
    "• The preprocess_document_bow function deduplicates tokens from each document and sorts them by their corresponding TF-IDF score.\n",
    "• The resulting ordered token string is then tokenized (again) to produce input IDs suitable for LegalBERT.\n",
    "\n",
    "Finally, these inputs are fed into the model for classification.\n",
    "\n",
    "• This variant does not modify the internal architecture of LegalBERT; it only changes the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"victorambrose11/normalized_scotus\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of documents in training dataset is 37956\n",
      "The lengthy document in the dataset contains 584365 number of tokens\n"
     ]
    }
   ],
   "source": [
    "highest=0\n",
    "total_length=0\n",
    "for idx in range(len(dataset['train'])):\n",
    "    total_length+=len(dataset['train'][idx]['text'])\n",
    "    if len(dataset['train'][idx]['text']) > highest:\n",
    "        highest=len(dataset['train'][idx]['text'])\n",
    "print (f'The average length of documents in training dataset is {round(total_length/len(dataset['train']))}\\nThe lengthy document in the dataset contains {highest} number of tokens')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Set this to avoid tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def initialize_model():\n",
    "    model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "def compute_tfidf_dict(documents, tokenizer):\n",
    "    print(\"Tokenizing documents...\")\n",
    "    # Main progress bar for tokenization\n",
    "    pbar_token = tqdm(total=len(documents), desc=\"Tokenizing\", position=0)\n",
    "    tokenized_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        tokenized_docs.append(tokenizer.tokenize(doc))\n",
    "        pbar_token.update(1)\n",
    "    pbar_token.close()\n",
    "    \n",
    "    def identity_tokenizer(text):\n",
    "        return text\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        tokenizer=identity_tokenizer,\n",
    "        preprocessor=lambda x: x,\n",
    "        lowercase=False\n",
    "    )\n",
    "    \n",
    "    print(\"Computing TF-IDF matrix...\")\n",
    "    with tqdm(total=1, desc=\"TF-IDF Computation\", position=0) as pbar_tfidf:\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(tokenized_docs)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        pbar_tfidf.update(1)\n",
    "    \n",
    "    return dict(zip(feature_names, tfidf_vectorizer.idf_))\n",
    "\n",
    "def process_documents_sequential(documents, tokenizer, batch_size=1000):\n",
    "    # Compute TF-IDF dictionary once\n",
    "    idf_dict = compute_tfidf_dict(documents, tokenizer)\n",
    "    \n",
    "    processed_docs = []\n",
    "    total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Create progress bars\n",
    "    main_pbar = tqdm(total=len(documents), desc=\"Overall Progress\", position=0)\n",
    "    batch_pbar = tqdm(total=total_batches, desc=\"Batch Progress\", position=1, leave=False)\n",
    "    \n",
    "    try:\n",
    "        # Process in batches\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            \n",
    "            # Process each document in the batch\n",
    "            for doc in batch:\n",
    "                # Tokenize\n",
    "                tokens = tokenizer.tokenize(doc)\n",
    "                \n",
    "                # Create dictionary of unique tokens and their scores\n",
    "                unique_tokens_dict = {token: idf_dict.get(token, 0) for token in set(tokens)}\n",
    "                \n",
    "                # Sort tokens by score\n",
    "                ordered_tokens = sorted(\n",
    "                    unique_tokens_dict.keys(),\n",
    "                    key=lambda x: unique_tokens_dict[x],\n",
    "                    reverse=True\n",
    "                )[:512]  # max_length=512\n",
    "                \n",
    "                processed_docs.append(\" \".join(ordered_tokens))\n",
    "                main_pbar.update(1)\n",
    "            \n",
    "            # Update batch progress\n",
    "            batch_pbar.update(1)\n",
    "            current_batch = i // batch_size + 1\n",
    "            tqdm.write(f\"Completed batch {current_batch}/{total_batches}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close progress bars\n",
    "        main_pbar.close()\n",
    "        batch_pbar.close()\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialization:   0%|          | 0/1 [00:00<?, ?it/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initialization: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|          | 0/5000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4363 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing: 100%|██████████| 5000/5000 [01:24<00:00, 59.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TF-IDF matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF Computation: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n",
      "                                                                    \n",
      "Overall Progress:  20%|██        | 1012/5000 [00:16<00:46, 85.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \n",
      "Overall Progress:  40%|████      | 2010/5000 [00:32<00:47, 62.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "Overall Progress:  60%|██████    | 3015/5000 [00:48<00:34, 57.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "Overall Progress:  80%|████████  | 4001/5000 [01:10<00:29, 34.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "Overall Progress: 100%|██████████| 5000/5000 [01:36<00:00, 51.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch 5/5\n",
      "\n",
      "Processing completed!\n",
      "\n",
      "Processed Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1400\n",
      "    })\n",
      "})\n",
      "\n",
      "Example Comparison (first document):\n",
      "Original text:\n",
      "329 U.S. 29 67 Supreme Court Reporter 1 91 L.Ed. 22 CHAMPLIN REFINING COv.UNITED STATES et al. Numbe...\n",
      "\n",
      "Processed text:\n",
      "##arro cim appel uncle ##mba inventorie sleeping val cov refiner rack understands champ enclosed hut...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "print(\"Initializing model and tokenizer...\")\n",
    "with tqdm(total=1, desc=\"Initialization\", position=0) as pbar:\n",
    "    tokenizer, model = initialize_model()\n",
    "    pbar.update(1)\n",
    "\n",
    "# Process documents sequentially\n",
    "processed_docs = process_documents_sequential(\n",
    "    documents=dataset['train']['text'],\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Get the original label feature\n",
    "original_label_feature = dataset['train'].features['label']\n",
    "\n",
    "# Define consistent features\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"label\": original_label_feature\n",
    "})\n",
    "\n",
    "# Create new dataset with processed texts\n",
    "new_train_dict = {\n",
    "    \"text\": processed_docs,\n",
    "    \"label\": dataset['train']['label']\n",
    "}\n",
    "\n",
    "# Create new dataset with the consistent features\n",
    "train_with_features = Dataset.from_dict(\n",
    "    new_train_dict,\n",
    "    features=features\n",
    ")\n",
    "\n",
    "# Update the dataset\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_with_features,\n",
    "    'test': dataset['test'],\n",
    "    'validation': dataset['validation']\n",
    "})\n",
    "\n",
    "# Print information about the processed dataset\n",
    "print(\"\\nProcessing completed!\")\n",
    "print(\"\\nProcessed Dataset Structure:\")\n",
    "print(new_dataset)\n",
    "\n",
    "# Print example comparison\n",
    "print(\"\\nExample Comparison (first document):\")\n",
    "print(\"Original text:\")\n",
    "print(dataset['train']['text'][0][:100] + \"...\")\n",
    "print(\"\\nProcessed text:\")\n",
    "print(new_dataset['train']['text'][0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 85.02ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  5.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  6.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/victorambrose11/lex_glue_normalized_TFIDF-SRT/commit/470e3ccf5d892fa07db790e4ff3510112ccaae7c', commit_message='Upload dataset', commit_description='', oid='470e3ccf5d892fa07db790e4ff3510112ccaae7c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/victorambrose11/lex_glue_normalized_TFIDF-SRT', endpoint='https://huggingface.co', repo_type='dataset', repo_id='victorambrose11/lex_glue_normalized_TFIDF-SRT'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to hugging face=\n",
    "new_dataset.push_to_hub(\"victorambrose11/lex_glue_normalized_TFIDF-SRT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
