{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada17c56-8f1c-41e9-9946-9982ea659693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 10:30:31.990010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[legal-bert-base-uncased] Epoch 1: Loss = 770.8020, Accuracy = 0.7086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[legal-bert-base-uncased] Epoch 2: Loss = 409.8816, Accuracy = 0.7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[legal-bert-base-uncased] Epoch 3: Loss = 265.8586, Accuracy = 0.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[legal-bert-base-uncased] Epoch 4: Loss = 164.1363, Accuracy = 0.7593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist5/miniconda3/envs/tamil/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[legal-bert-base-uncased] Epoch 5: Loss = 98.3053, Accuracy = 0.7764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m         metrics_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: precision, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1})\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m train_model(model_legalbert, tokenizer_legalbert, dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegal-bert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m train_model(model_roberta, tokenizer_roberta, dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_tfidf_svm\u001b[39m(dataset):\n",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, dataset, model_name, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     73\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     74\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 75\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "# from datasets import DatasetDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.load_dataset(\"coastalcph/lex_glue\", \"scotus\")\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer_legalbert = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return self.fc(outputs.pooler_output)\n",
    "\n",
    "model_legalbert = TextClassifier(\"nlpaueb/legal-bert-base-uncased\", num_labels=13)\n",
    "model_roberta = TextClassifier(\"roberta-base\", num_labels=13)\n",
    "metrics_data = []\n",
    "\n",
    "def train_model(model, tokenizer, dataset, model_name, epochs=10, batch_size=8, lr=2e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_encodings = tokenizer(dataset[\"train\"][\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    val_encodings = tokenizer(dataset[\"validation\"][\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    train_labels = torch.tensor(dataset[\"train\"][\"label\"])\n",
    "    val_labels = torch.tensor(dataset[\"validation\"][\"label\"])\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], val_labels)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "        metrics_data.append({\"Model\": model_name, \"Epoch\": epoch+1, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1\": f1})\n",
    "        print(f\"[{model_name}] Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "train_model(model_legalbert, tokenizer_legalbert, dataset, \"legal-bert-base-uncased\")\n",
    "train_model(model_roberta, tokenizer_roberta, dataset, \"roberta-base\")\n",
    "\n",
    "def train_tfidf_svm(dataset):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(dataset[\"train\"][\"text\"])\n",
    "    X_val_tfidf = vectorizer.transform(dataset[\"validation\"][\"text\"])\n",
    "    svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "    svm_model.fit(X_train_tfidf, dataset[\"train\"][\"label\"])\n",
    "    y_pred = svm_model.predict(X_val_tfidf)\n",
    "    accuracy = accuracy_score(dataset[\"validation\"][\"label\"], y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(dataset[\"validation\"][\"label\"], y_pred, average='weighted')\n",
    "    metrics_data.append({\"Model\": \"TFIDF+SVM\", \"Epoch\": 10, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1\": f1})\n",
    "    print(f\"[TFIDF+SVM] Accuracy = {accuracy:.4f}\")\n",
    "    return svm_model, vectorizer\n",
    "\n",
    "train_tfidf_svm(dataset)\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=metrics_df, x=\"Epoch\", y=\"Accuracy\", hue=\"Model\", marker=\"o\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bbbd3-7584-4f98-a6aa-0bca03a05450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamil",
   "language": "python",
   "name": "tamil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
